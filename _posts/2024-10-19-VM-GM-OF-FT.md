---
layout: post
title: Vison Model 과 생성형 모델, 과적합과 파인튜닝
subtitle: TIL Day 27
cover-img: "/assets/img/background.png"
thumbnail-img: ''
share-img: ''
tags: [TIL, DL]
author: polaris0208
---

## ***Vision Model***
> CNN
> 잔차 연결
> ResNet-50, 101, 152 등 

### ***ResNet***
- 기울기 소실 등 기존 모델들의 수학적 문제에 대안 제시
#### 개념
1. ResNet
- 깊은 신경망 학습
- ***Residual Learning*** ; 잔차 학습 개념
``` 
기존 모델 : 깊게 학습 할 수록 input과 가중치가 희미해짐
ResNet: 입력과 출력의 차이인 "잔차"를 학습 , 최초 input은 그대로 계속 전달
```
- 모델명 뒤의 숫자는 레이어 개수
2. 특징
- 잔차학습으로 기울기 소실, 기울기 폭발 문제 해결
- 간단한 블록구조: 네트워크를 쉽게 확장

### 다른 모델
#### ***VGG***- 크고 단순
- 작은 3x3 필터 사용 - 깊이 증가
- 레이어 16, 19
- 단순하고 규칙적인 구조 - 다양한 변형 모델

#### ***Inception*** - 얕고 복잡 
- 네트워크 내의 네트워크 개념
- inception 블록 - ***깊이와 너비 동시에 증가***
  - 다양한 크기의 필터를 동시에 사용
  - 1x1 필터 사용 - 채널 수 감소 - 계산량 감소
  - 다양한하고 효율적인 학습

#### ***YOLO(You Only Look Once)***
- 객체 탐지 모델
- 이미지 속의 객체의 위치와 클래스를 동시에 예측
- ***이미지 전체를 한번에 탐지***
  - 단일 신경망(한번에 예측)
  - end to end(객체 탐지-분류가 한번에 이루어짐)
  - 전역 분석
``` 
S x S 크기의 그리드 셀(각 그리드 셀 중 한개만 객체 포함)
B 개의 경계 상자 
- 중심 좌표(x,y) 
- 상대 크기(w,h)
- cofindence(상자의 정확도-상자가 객체를 포함할 확률)
C 개의 클래스에 대한 확률 분포 예측

각 그리드 셀 : 각 경계상자 컨피던스 점수 x 클래스 확률 
최종점수 계산 -> 점수가 높은 상자 최종 예측
non-Maximum suppression 통해 여러개의 상자가 하나의 객체 탐지하는 경우 중복 제거
```
- 저작권 문제로 버전 사용에 주의가 필요

### ***Image Segmentation***
- 이미지를 픽셀 단위로 분해해서 각 픽셀이 어느 객체에 속하는지 분석
- ***Sementic Segmentation*** : 이미지의 각 픽셀을 클래스 레이블로 구분 ex) 사람인지 구분
- ***Instance Segmentation*** : 클래스 내에서도 개별 객체를 구분 ex) 사람 중에 각각의 사람을 구분
- ***FCN(Fully Convolution Network)***
  - 모든 레이어가 CNN으로 구성(완전연결레이어 제거)
  - 입력크기에 상관없이 구동
  - 픽셀 단위 분류 수행
  - 기존 ***CNN의 마지막 레이어을 fc에서 conc 로 바꾸고 업스케일링*** = 원본크기 출력
  - 시멘틱 세그멘테이셔에 사용
- ***U-Net***
  - FCN의 발전 모델
  - 생물학적 세포 이미지 세그멘테이션 목적
  - 인코더(특징추출) - 스킵 연결(인코더의 높은 해상도 특징을 전달) - 디코더(원본 크기로 복원)
  - 소량의 데이터에서 높은 성능 - 의료이미지 처리에 사용
- ***Mask R-CNN***
  - 객체 검출, 인스턴스 세그멘테이션 동시 수행
  - 다양한 어플리케이션에서 사용

  -------
## Auto Encoder
>모든 생성형 모델에 포함
>인코더 - 디코더 구조 
### 개념
1. 기본 개념
- 입력 데이터 압축 - 복원 방식
- 비지도학습
- 차원 축소, 잡음 제거, 생성 모델
2. 동작원리
- 인코더: 입력 데이터를 저차원 `latent space`로 표현
  - 목적: 중요한 특징 추출, 입력 데이터 압축
- 디코더: 저차원 표현을 고차원 데이터로 복원
  - 목적: 최대한 원본과 가깝게 복원
- 잠재공간`latent space`: 인코더에 의해 생성된 저차원 공간
  - 입력 데이터의 중요한 데이터만 포함
### 구조
1. 종류
- 기본 오토 인코더
- 변형 오토 인코더
  - 딥 오토 인코더`Deep Autoencoder`
    - 깊은 레이어
    - 더 복잡한 데이터 표현 학습
    - 오토 인코더가 딥러닝에 사용되면 딥 오토 인코더
  - 변분 오토인코더`VAE: Variational Autoencoder`
    - 확률적 잠재 공간을 사용
    - 형태를 학습하고 이를 바탕으로 새로운 데이터 생성
    - 예시) 사람의 형태를 학습 -> 사람 형태의 데이터 생성
  - 희소 오토인코더`Sparse Autoencoder`
    - 잠재공간을 희소하게 유지
    - 중요한 특징만 학습
  - 잡음 제거 오토인코더`Denoising Autoencoder`
    - 입력 데이터에 잡음 추가
    - 잡음을 제거하는 학습을 통해 복원 능력 향상
    
## 생성형 모델
> GAN, VAE

### ***GAN***
>Generative Adversarial Network
#### 개념
- 두개의 모델이 서로 경쟁
- 이미지 생성, 데이터 증강, 스타일 변화 등 분야
- conditional GAN : 특정 조건을 기반으로 데이터 생성
- unconditional GAN : 무작위 데이터 생성

#### 구조
1. 생성자 ***Generator***
- ***random input***을 받아 새로운 데이터 생성
2. 판별자 ***Discriminator***
- 생성자가 생성한 데이터 ***Sample***이 진짜인지 판별
3. 생성자와 판별자가 경쟁하는 과정에서 성능 향상

#### 단점
- 경쟁이 적적히 조율되지 않으면 학습이 진행되지 않는 문제 발생
- 모드 붕괴: 생성자가 데이터를 제한적으로만 생성하는 문제 발생
- ***DCGAN, WGAN, CycleGAN*** 등의 변형 모델에서 개선

### ***VEA***
> ***Variational Encoder***

#### 개념
- ***잠재공간을 통계적인 기법***으로 생성
- 확률분포를 모델링 
- 데이터의 생성과 분포를 효과적으로 학습
- 일반적인 지식을 학습

#### 구조
- 기본 오토인코더와 동일
- 잠재공간 생성에서 차이
- VAE는 잠재변수를 샘플링

#### 동작원리
- 인코더
  - 입력 데이터 x
  - 잠재변수 z에 평균과 분산으로 맵핑
  - 잠재변수는 정규분포에서 샘플링
  - ***Reparameterization Trick***
    - 미분하지 못하는 샘플링 데이터의 파라미터를 조정하는 과정
- 디코더
  - 잠재변수를 입력 데이터로 받음
  - 두가지 손실 함수로 구성
    - ***Reconstruction Loss*** : 원래 데이터와 복원 데이터의 차이를 최소화
    - ***Kullback-Leibler Divergence*** : 인코더가 학습한 잠재분포와 정규분포의 차이를 계산
      - [참고](https://daebaq27.tistory.com/88) P분포 대신 Q분포를 사용할 경우 정보량 변화를 측정

#### 단점
- 이해하기 어려움
- 잠재공간 분포 학습이 어려움
------
## 전이 학습
> Transfer Learning
> Fine-tuning과는 구별

### 개념
1. 기본 개념
- 이전에 학습한 지식을 ***전이하여***  새로운 모델에 적용
- 일부 레이어를 미세 조정하거나 추가해서 사용
2. 필요성
- 데이터 부족: 데이터가 불충분 할 때 기존모델의 지식 활용
- 학습시간 단축: 처음부터 모델을 학습하는 것보다 빠름
- 성능 향상: 기존모델은 대규모 데이터를 통해 학습 = 고성능

3. 원리
- 특징 추출기 ***Feature Extractor***
  - 초기층 : 사전 학습 모델로 고정
  - 마지막층 : 새로운 데이터에 맞게 재학습
- 미세 조정 ***Fine Tuning***
  - 전체를 새로운 데이터에 맞게 재학습

4. 과정
- 사전 학습된 모델 로드
  - PyTorch에서 제공
  - ***ResNet,, VGG, Inception***
- 모델 수정
  - 마지막 층을 새로운 문제에 맞게 수정
  - 초기층은 기존 모델로 고정
- 모델 학습
  - 새로운 데이터에 맞게 학습
  - ***특징 추출기 또는 미세조정 방식 중 선택***

  ## 과적합 방지 기법
> 정규화, Dropout, 조기종료, 데이터 증강
- 과적합 **Overfitting**
- 학습이 과도하게 진행되어 일반적인 문제 예측 불가
- 답을 완전히 외운 상태

### 정규화 **Nomalization**
1. 개념
- 데이터를 **일정한 범위**로 조정 
2. 종류
- 배치 정규화 **Batch Nomalization**
  - 각 미니배치에서 활성화 값을 정규화
  ``` 
  1. 미니 배치에서 특징의 평균과 분산을 계산
  2. 활성화 값은 평균과 분산을 이용해 정규화
  3. 재스케일링 및 시프트 
  - 학습가능한 파라미터 감마, 베타 사용
  - 값을 조정
  ```
  - 활성화 값: 활성화 함수를 통해 계산된 값
  - 미니배치: 적절한 크기로 나눠 모델을 업데이트
  - 장점
    - 학습이 빨라짐
    - 학습률을 더 크게 사용
    - 학습이 안정적이고, 초기화 값에 민강하지 않게 됨
    <br>
- 레이어 정규화 **Layer Nomalization**
  - 주로 RNN 사용
  - 각 레이어의 활성화 값을 정규화 
    - 순환하는 RNN의 특징으로 정규화가 일부 제한 떄문
  - 동작원리
  ```
  1. 각 레이어의 활성화 값을 확인
  2. 평균과 분산을 확인
  3. 이 값을 통해 정규화 진행
  4. 재스케일링 시프트
  ```
  - 배치 크기에 영향을 받지 않음 
    - 시퀀스 데이터에 적합
    <br>
- 그룹 정규화 **Group Nomalization** 
  - 배치 정규화와 레이어 정규화의 장점을 결합
  - 채널을 그룹으로 나누고 각 그룹에서 정규화
  <br>
- 인스턴스 정규화 **Instance Nomalization**
  - 각 샘플에 대해서 정규화

> 그룹 정규화와 인스턴스 정규화는 Advanced 모델
> 상세하게 공부한 후 사용
> 비전 작업, 스타일 변환, 생성 작업 등
### 드롭 아웃 **Dropout**
- 학습 과정에서 무작위로 뉴런을 **비활성화**

```
<과적합>
30 명의 학생
1명의 뛰어난 학생이 A존재 
모든 문제를 A가 해결하는 문제 발생
A가 모르는 문제는 해결 불가
<드롭아웃>
임의의 학생들에게 문제를 배정
학생 전체가 일반적인 지식을 학습
한 학생이 모르는 문제는 다른 학생들이 해결 가능하도록 함
```

- 평가 또는 실사용 시에는 모든 뉴런을 활성화

### 조기 종료 **Early Stopping**
- **학습 - 검증 - 평가** 데이터로 분할
- 검증 데이터를 통해 성능을 판단
- 검증 데이터의 성능이 더이상 오르지 않으면 중단

### 데이터 증강 **Data Augmentation**
- 원본 데이터를 변형하여 **새로운 데이터 생성**
- 데이터를 **다양화**
- 이미지와 같이 변형할 가능성이 많은 경우 효과적

``` 
<원본 데이터>
나는 - 고등학생 탐정 - 남도일
<변형 데이터>
나는 - 남도일 - 고등학생 탐정
<변형 데이터>
고등학생 탐정 - 나는 - 남도일
....
```
-----
## 하이퍼 파라미터 튜닝<br>**Hyperparameter Tuning**
> 모델 학습의 여러가지 설정값
> 설정에 따라 모델 성능에 영향



### 하이퍼 파라미터
1. 학습률 **Learining Rate**
- 가중치를 업데이트 시 **얼마나 학습할지** 결정 
- 일반적으로 0.1, 0.01, 0.001 등

2. 배치 크기 **Batch Size**
- 한번의 업데이트에 사용하는 데이터의 샘플 크기
- 일반적으로 2의 배수꼴
- 32, 64, 128

3. 에포크 **Epoch**
- 데이터를 반복해서 학습하는 **한 주기**
- 10 에포크 = 주기를 10회 
- 조기 종료 기법을 사용하여 적절하게 결정 가능

4. 모멘텀 **Momentum**
- 이전 기울기를 현재 기울기에 반영
  - 학습 속도를 높이고, 진동의 줄임
- 일반적으로 0.9, 0.99 등
- 최적화 방법 설정에 사용
  - 학습결과를 바탕으로 가중치를 어떻게 업데이트할지 결정

```
경사하강법: 손실함수를 최소화
- 모델의 가중치를 반복적으로 업데이트
- 각 업데이트는 손실함수의 기울기를 보고 미분적으로 진행
```

- **Batch Gradient Descent**
  - 전체 데이터 셋 활용 
  - 한번에 가중치를 업데이트
- **Stochastic Gradient Descent**
  - 데이터 샘플을 사용
  - 가중치 업데이트가 균일하지 못함
  - 손실함수 진동 가능
- **Mini Batch Gradient Descent**
  - 데이터 셋을 작은 배치로 나눠서 사용
  - 효율성과 안정성

- **AdaGrad**
  - 자주 등장하는 특징은 학습률을 줄임
  - 드물게 등장하는 특징의 학습률을 늘림
  - 학습이 장기화 되면 학습률이 줄어드느 문제 방생

- **RMSProp**
  - 최근의 기울기를 기반으로 학습률을 조정
  - AdaGrad 단점 보완
  - 자체적인 하이퍼파라미터가 많아 설정이 복잡힌 문제

- **Adam**
  - 모멘텀과 RMSProp의 장점을 결합
  - 학습률을 동적으로 조정
  - 빠르고 안정적
  - 하이퍼파라미터가 많아 설정이 어려움

4. 가중치 초기화 **Weight Initialization**
- 모델의 가중치를 초기화하는 방법을 결정
- **Xavier** 초기화
  - 입력 노드의 수를 고려
  - **Sigmoid**, **Hyperbolic Tangent**활성화 함수 사용하는 경우
- **He** 초기화
  - 입력과 출력 노드의 수를 고려
  - **ReLU** 함수 사용하는 경우

### 자동 튜닝 기법
1. **Grid Search**: 모든 조합을 시도

2. **Random Search**: 무작위 값을 선택

3. **Bayesian Optimization**: 이전 평가 결과를 바탕

----
## 모델 평가와 검증<br>**Validation**
> 교차 검증 방식
### 개념
- 교차검증 **Cross-Validation**
  - 모델의 성능 평가
  - 데이터를 여러번 나누어 학습과 검증을 반복
- 필요성
  - 과적합 방지
  - 일반화 성능 평가
  - 데이터 효율성
### K-Fold 교차 검증
1. 원리
- 데이터를 k개의 폴더로 분리
- 각 폴드가 한번씩 검증 데이터로 사용
- 나머지는 학습 데이터
- 검증 결과를 평균하여 모델을 평가
2. 적용방법
- 각 폴드에 대한 검증 결과 저장
- 저장한 결과를 평균하여 성능 평가
----